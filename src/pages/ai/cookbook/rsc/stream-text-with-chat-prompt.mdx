import Layout from "@/components/Layout";
import Button from "@/components/Common/button";
import Section from "@/components/Common/section";
import Alert from "@/components/Common/alert";
import ThemePlatformIcon from "@/components/Common/themeIcons"
import Tabs from "@/components/Common/tab";
import Step from "@/components/Common/step";
import Card from "@/components/Common/card";
import Important from "@/components/Common/important";
import Highlight from "@/components/Common/highlight";
import Link from "next/link";
import PlatformIcon from "@/components/Common/icons";
import HighlightTabs from "@/components/Common/HighlightTabs";
import IconContainer from "@/components/Common/IconContainer";
import {
  GoContainer,
  GoDatabase,
  GoRocket,
  GoServer,
  GoMail,
  GoGlobe,
  GoArrowLeft,
  GoTelescope,
} from "react-icons/go";

import Head from "next/head";

<Layout>
<Head>
<title>مستندات استریم متن با ورودی با AI با ساخت RSC - لیارا</title>
<meta property="og:title" content="مستندات خدمات رایانش ابری لیارا" />
<meta property="og:description" content="مستندات مربوط به نحوه استریم متن با ورودی با استفاده از سرویس هوش مصنوعی لیارا به وسیله ساخت کامپوننت سرور ری‌اکت"  />
<meta property="og:image" content="https://media.liara.ir/logos/liara-poster.jpg" />
</Head>


# استریم متن با ورودی با هوش مصنوعی با ساخت RSC
<hr className="mb-2" />


<Alert variant="info">
<p>
این مثال از React Server Components (یا همان RSC)، استفاده می‌کند. اگر که قصد دارید از 
client side rendering و هوک‌ها استفاده کنید؛ می‌توانید به <a href="/ai/cookbook/nextjs/stream-text-with-chat-prompt" className="text-[#2196f3]">این مستندات</a> مراجعه کنید. 
</p>
</Alert>
<div className="h-2" />

chat completion گاهی ممکن است زمان‌بر باشد، مخصوصاً زمانی که پاسخ بزرگ و طولانی است.
در چنین مواردی، استریم chat completion به‌صورت بلادرنگ به سمت کلاینت می‌تواند مفید باشد.
این کار به کلاینت اجازه می‌دهد که پیام جدید را در حین تولید توسط مدل، دریافت کند، به‌جای اینکه کاربر منتظر بماند تا تولید پاسخ، به پایان برسد.


<hr className="mb-2" />
<Section id='client' title='کلاینت' />

بیایید یک فضای مکالمه‌ای ساده میان یک کاربر
و LLM ایجاد کنیم و یک دکمه کار بذاریم که <Important>continueConversation</Important> را فراخوانی خواهد کرد. 

<div className="h-2" />

در مسیر <Important>app/page.tsx</Important>، قطعه کد زیر را قرار دهید: 

<div className="h-2" />
<div dir='ltr'>
    <Highlight className="js">
        {`// npm i @ai-sdk/rsc

'use client';

import { useState } from 'react';
import { Message, continueConversation } from '@/app/actions';
import { readStreamableValue } from '@ai-sdk/rsc';

// Allow streaming responses up to 30 seconds
export const maxDuration = 30;

export default function Home() {
  const [conversation, setConversation] = useState<Message[]>([]);
  const [input, setInput] = useState<string>('');

  return (
    <div>
      <div>
        {conversation.map((message, index) => (
          <div key={index}>
            {message.role}: {message.content}
          </div>
        ))}
      </div>

      <div>
        <input
          type="text"
          value={input}
          onChange={event => {
            setInput(event.target.value);
          }}
        />
        <button
          onClick={async () => {
            const { messages, newMessage } = await continueConversation([
              ...conversation,
              { role: 'user', content: input },
            ]);

            let textContent = '';

            for await (const delta of readStreamableValue(newMessage)) {
              textContent = \`\${textContent}\${delta}\`;

              setConversation([
                ...messages,
                { role: 'assistant', content: textContent },
              ]);
            }
          }}
        >
          Send Message
        </button>
      </div>
    </div>
  );
}`}
    </Highlight>
</div>
<hr className="mb-2" />
<Section id='server' title='سرور' />

اکنون، بیایید تابع <Important>continueConversation</Important> را پیاده‌سازی کنیم که پیام کاربر را درون فضای مکالمه، قرار می‌دهد
و جواب مدل را به کاربر به‌صورت استریمی، برمی‌گرداند.

<div className="h-2" />
در مسیر <Important>app/actions.ts</Important>، قطعه کد زیر را قرار دهید:

<div className="h-2" />
<div dir='ltr'>
    <Highlight className="js">
        {`// npm i @ai-sdk/openai@^1 ai@^4
'use server';

import { streamText } from 'ai';
import { createOpenAI } from '@ai-sdk/openai';
import { createStreamableValue } from '@ai-sdk/rsc';

const my_model = createOpenAI({
  baseURL: process.env.BASE_URL!,
  apiKey: process.env.LIARA_API_KEY!,
});

export interface Message {
  role: 'user' | 'assistant';
  content: string;
}

export async function continueConversation(history: Message[]) {
  'use server';

  const stream = createStreamableValue();

  (async () => {
    const { textStream } = streamText({
      model: my_model('openai/gpt-4o-mini'),
      system:
        "You are a dude that doesn't drop character until the DVD commentary.",
      messages: history,
    });

    for await (const text of textStream) {
      stream.update(text);
    }

    stream.done();
  })();

  return {
    messages: history,
    newMessage: stream.value,
  };
}`}
    </Highlight>
</div>
<div className="h-2" />

<Alert variant="info">
<p>
متغیرهای محیطی <Important>BASE_URL</Important> و <Important>LIARA_API_KEY</Important> همان baseUrl <a href="https://liara.ir/products/ai/" className="text-[#2196f3]">سرویس هوش مصنوعی لیارا</a> و <a href="/references/api/about/#api-access-key" className="text-[#2196f3]">کلید API لیارا</a> هستند که باید در بخش متغیرهای محیطی برنامه خود، آن‌ها را تنظیم کنید. 
</p>
</Alert>

<Alert variant="success">
<p>
پروژه فوق را می‌توانید به‌صورت کامل در <a href="https://github.com/liara-cloud/ai-sdk-examples/tree/master/RSC/stream-text-with-chat-prompt" className="text-[#2196f3]">گیت‌هاب لیارا</a>، مشاهده کنید. 
</p>
</Alert>

</Layout>